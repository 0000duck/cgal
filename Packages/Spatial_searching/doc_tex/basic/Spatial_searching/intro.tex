\chapter{Approximate Spatial Searching}

\section{Introduction}

The {\bf approximate spatial searching} package implements
exact and approximate distance browsing
by providing implementations of algorithms supporting

\begin{itemize} 

\item
both nearest and furthest neighbour searching

\item
both exact and approximate searching

\item 
(approximate) $k$-nearest and $k$-furthest neighbour searching

\item 
(approximate) incremental nearest and incremental furthest neighbour searching

\item
query items representing points and spatial objects.

\end{itemize}

In these searching problems a set $P$ of data points in $d$-dimensional
space is given.
These points are preprocessed into a $k$-$d$ tree data structure, so that given
any query item $q$ the points of $P$ can be browsed efficiently.
ASPAS is designed for data sets that are small enough to store
the search structure in main menory (in contrast to approaches
from databases that assume that the data reside in secondary storage).

\subsection{Incremental Nearest Neighbour Searching and Distance Browsing}

Spatial searching supports browsing through a collection of spatial objects
stored in a spatial data structure on the basis of their distances to an arbitrary spatial
query object. The first approach is one that makes use of a $k$-nearest neighbour
algorithm, where $k$ is know prior to the invocation of the algorithm.
Hence, the number of nearest neighbours has to be
guessed. If the guess is too large redundant computations are performed.
If the number is too small the computation has to be  
reinvoked for a larger number of neighbours, thereby performing redundant computations.
The second approach is incremental in the sence that having obtained
the $k$ nearest neighbours, the $k$ + 1$^{st}$ neighbour can be obtained without
having to calculate the $k$ + 1 nearest neighbour from scratch.
The incremental approach is useful when processing queries where
one but not all of the conditions involves spatial proximity (e.g. the nearest city to Paris with
population greater than a million).

\subsection{Furthest Neighbour Searching}

With relatively minor modifications, nearest neighbour searching algorithms can be
used to find the furthest object from the query object. Furthest neighbour searching is invoked if
a boolean \ccc{Search_nearest} is set to false in the constructor of the searching classes.
If this boolean is set to true, or is not set, nearest neighbour searching is invoked.

\subsection{Range Searching}

Range queries are supported by using incremental nearest neighbour search
using iso-rectangles as query item, and using a $L1$-distance between iso-rectangles and points.
This distance is defined such that all points contained in a query item have distance zero
to the query. Hence, an incremental nearest neighbour search will report first all points
contained in the query item.  

\subsection{Approximate Searching}

The approximate spatial searching package supports both exact and approximate browsing, by
specifying an approximation facter $\epsilon \geq 0$ with the query.

\subsection{The $k$-$d$ tree}

Bentley \cite{b-mbstu-75} introduced the $k$-$d$ tree as a generalization of the binary
search tree in higher dimensions. $k$-$d$ trees hierarchically decompose space into a
relatively small number of rectangles such that no rectangle contains too many input objects.
For our purposes, a {\it rectangle} in real $d$ dimensional space, $\R^d$, is the product of $d$ closed
intervals on the coordinate axes.
$k$-$d$ trees are obtained by partitioning point sets in $\R^d$ using
($d$-1)-dimensional hyperplanes.
Each node in the tree is split into two children by one such separating hyperplane.

Each internal node of the $k$-$d$ tree is associated with a rectangle
and a hyperplane orthogonal to
one of the coordinate axis, which splits the rectangle into two parts.
Therefore, such a hyperplane, defined by a splitting dimension
and a splitting value, is called a separator.
These two parts are then associated
with the two child nodes in the tree. The process of partitioning space continues until the number of data
points in the rectangle falls below some given treshold. The rectangles associated with the leaf nodes
are called {\it buckets}, and they define a subdivision of the space into rectangles.
Data points are only stored in the leaf nodes of the tree, not in the internal nodes.

Friedmann, Bentley and Finkel \cite{fbf-afbml-77} described the standard
search algorithm to find the $k$th nearest neighbour by searching a $k$-$d$ tree recursively.

When encountering a node of the kd-tree the algorithm first visits the child that is closest
to the query point. On return, if the rectangle containing  the other child lies within
1/ (1+$\epsilon$) times the distance to the $k$th nearest neighbours so far, then
the other child is visited recursively.
Priority search \cite{am-annqf-93} visits the nodes in increasing order of distance from
the queue with help of a priority queue.
The search stops when the distance of the query point to the nearest nodes
exceeds the distance to the nearest point found with a factor 1/ (1+$\epsilon$).
Priority search supports next neighbour search, standard search does not.

\subsection {Splitting Rules}

ASPAS provides the following different splitting rules,
which determine how a separating hyperplane is selected:

\begin{itemize}

\item {\bf Midpoint of rectangle}
This splitting rule cuts a rectangle through its midpoint orthogonal
to the longest side.

\item {\bf Midpoint of max spread}
This splitting rule cuts a rectangle through $Mind+Maxd/2$ orthogonal
to the dimension with the maximum point spread $[Mind,Maxd]$.

\item {\bf Sliding midpoint}

This is a modification of the midpoint of rectangle splitting rule.
It first attempts to perform a midpoint of rectangle split as
described above. If data points lie on both sides of the separating
plane the sliding midpoint rule computes the same separator as
the midpoint of rectangle rule. If the data points lie only on one
side it avoids this by sliding the separator, computed by
the midpoint of rectangle rule, to the nearest datapoint.

\item {\bf Median of rectangle}

The splitting dimension is the dimension of the longest side of the rectangle.
The splitting value is defined by the median of the coordintes of the data points
along this dimension.

\item {\bf Median of max spread}

The splitting dimension is the dimension of the longest side of the rectangle.
The splitting value is defined by the median of the coordinates of the data points
along this dimension.

\item {\bf Fair}
This splitting rule is a compromise between the median of rectangle splitting rule
and the midpoint of rectangle splitting rule. This splitting rule maintains an upper
bound on the maximal allowed ratio of the longest and shortest side of
a rectangle (the value of this upper bound is set in the constructor of the
fair splitting rule). Among the splits that satisfy this bound, it selects
the one in which the points have the largest spread.
It then splits the points in the most even manner possible, subject
to maintaining the bound on the ratio of the resulting rectangles.

\item {\bf Sliding fair}
This splitting rule is a compromise between the fair splitting rule
and the sliding midpoint rule.
Sliding fair-split is based on the theory that there are
two types of splits that are good: balanced splits that
produce fat rectangles, and unbalanced splits provided
the rectangle with fewer points is fat.

Also, this splitting rule maintains an upper
bound on the maximal allowed ratio of the longest and shortest side of
a rectangle (the value of this upper bound is set in the constructor of the
fair splitting rule). Among the splits that satisfy this bound, it selects
the one one in which the points have the largest spread.
It then considers the most extreme cuts that would be allowed by the
aspect ratio bound. This is done by dividing the longest side of
the rectangle by the acpect ratio bound. If the median cut lies
between these extreme cuts, then we use the median cut. If not,
then consider the extreme cut that is closer to the median.
If all the points lie to one side of this cut, then we slide the cut
until it hits the first point.
This may violate the aspect ratio bound, but will never generate empty cells.

\end{itemize}

Also, a user of ASPAS may provide an implementation of his own
splitting rule.

\section{Software Design}

The approximate spatial searching package consists of the following classes:

\begin{itemize}

\item
The class \ccc{Kd_tree_rectangle} implements $d$-dimensional iso-rectangles, methods to compute bounding boxes
of point sets and methods to split iso-rectangles.

\item
The classes \ccc{Kd_tree} and \ccc{Kd_tree_node} implement the $k$-$d$ tree.

\item
The class \ccc{Kd_tree_traits_point} provides parameters for the construction of the $k$-$d$ tree.

\item
The class \ccc{Plane_separator} implements a separator.

\item 
The class \ccc{Point_container} is a point container providing a method, that given a separator
splits a point set. Also \ccc{Point_container} provides methods that support the implementation
of splitting rules.

\item
The function object classes \ccc{Midpoint_of_rectangle}, \ccc{Midpoint_of_max_spread},
\ccc{Sliding_midpoint}, \ccc{Median_of_rectangle}, \ccc{Median_of_max_spread}, \ccc{Median_of_max_spread},
\ccc{Fair} and \ccc{Sliding_fair} implement the splitting rules.

\item
The class \ccc{Weighted_Minkowski_distance} implements a weighted $L_p$-distance for $d$-dimensional points.

\item
The class \ccc{L1_distance_rectangle_point} implements a $L1$ distance for $d$-dimensional
iso-rectangles and points. 

\item
The class \ccc{General_standard_search} implements the standard search strategy for general distances
like the $L1$ distance for iso-rectangles.

\item
The class \ccc{General_priority_search} implements the priority search strategy for general distances
like the $L1$ distance for iso-rectangles.

\item
The class \ccc{Orthogonal_standard_search} implements the standard search strategy for orthogonal distances
like a weighted $L_p$ distance.

\item
The class \ccc{Orthogonal_priority_search} implements the priority search strategy for general distances
like the $L1$ distance for iso-rectangles.

\end{itemize}

\begin{ccTexOnly}

\begin{figure} [t]
  \begin{center}
  \leavevmode
  \vspace*{6cm}
  \hspace*{-2cm}
  \scalebox{.7}{\includegraphics{Fig1.eps}}
  \end{center}
  \vspace*{-8cm}
  \caption{Overiew of use of traits classes.}
\end{figure}

\end{ccTexOnly}

%for html
%\lcHtml{\label{KDT_fig:kdtree}}
\begin{ccHtmlOnly}
<P>
<center><img border=0 src="Fig1.gif" alt=" "><br>
Overview of use of traits classes</center>
\end{ccHtmlOnly}

\section{Example Programs}

\subsection{K-Nearest Neighbour Searching}

{\bf Example program to be extended.}

The first example program illustrates building $k$-$d$ trees using 1000
100-dimensional points.
The way the $k$-$d$ tree is build is defined by the traits variable \ccc{tr(bucket_size, s, 3.0 true)}.
In the example the \ccc{bucket_size} is set to 10.
Hence, each leaf nodes stores at most 10 points.
The strategy for splitting the nodes is defined by $s$.
Some rules use a maximal aspect ratio to define
the maximal allowed ratio between the largest and smallest side of a rectangle.
This aspect ratio is set to 3.0. The boolean value true indicates that
extended internal nodes are used.

\ccIncludeExampleCode{Example03.C}

\subsection{Incremental Nearest Neighbour Searching}

{\bf Example program to be provided.}

\subsection{Distance Browsing}

{\bf Example program to be provided.}

\subsection{Range Querying}

{\bf Example program to be provided.}
